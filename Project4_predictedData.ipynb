{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGjbfgm-2uOO",
        "outputId": "56b6fafc-1d9c-49ca-f54e-5a6b14b90653"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/meminfo | grep MemTotal"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56kaeiKi28VK",
        "outputId": "5223f184-ed99-40ca-ec11-d6ee44326d11"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MemTotal:       53470708 kB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGC70yewlWC3",
        "outputId": "539ccc37-3846-44b6-ef47-ed70700d26dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Fetched 257 kB in 2s (126 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model reloaded successfully!\n",
            "Cleaned data loaded successfully!\n",
            "+---+----------+----+-------------+----------+-----------------+-------------+---------------------------+----+-----+---+\n",
            "|FSA|      DATE|HOUR|CUSTOMER_TYPE|PRICE_PLAN|TOTAL_CONSUMPTION|PREMISE_COUNT|AVG_CONSUMPTION_PER_PREMISE|YEAR|MONTH|DAY|\n",
            "+---+----------+----+-------------+----------+-----------------+-------------+---------------------------+----+-----+---+\n",
            "|M1V|2024-07-01|   1|    SGS <50kW|    Tiered|           3125.6|         1716|         1.8214452214452215|2024|    7|  1|\n",
            "|L7M|2024-07-01|   1|    SGS <50kW|       TOU|           1563.8|          683|         2.2896046852122987|2024|    7|  1|\n",
            "|L9W|2024-07-01|   1|  Residential|    Tiered|            925.0|         1232|         0.7508116883116883|2024|    7|  1|\n",
            "|M2J|2024-07-01|   1|    SGS <50kW|    Tiered|            598.0|          267|         2.2397003745318353|2024|    7|  1|\n",
            "|K2C|2024-07-01|   1|  Residential|       TOU|           5095.2|         7714|         0.6605133523463832|2024|    7|  1|\n",
            "+---+----------+----+-------------+----------+-----------------+-------------+---------------------------+----+-----+---+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Set Spark version\n",
        "spark_version = 'spark-3.5.4'\n",
        "os.environ['SPARK_VERSION'] = spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"ReadBestModel\").getOrCreate()\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load the best trained model\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "# Define the custom metric\n",
        "def mse(y_true, y_pred):\n",
        "    return K.mean(K.square(y_pred - y_true), axis=-1)\n",
        "\n",
        "# Load the pre-trained best model\n",
        "best_model_path = \"/content/drive/My Drive/TeamFiles/best_neural_network.h5\"\n",
        "best_model = tf.keras.models.load_model(best_model_path, custom_objects={\"mse\": mse})\n",
        "print(\"Best model reloaded successfully!\")\n",
        "\n",
        "# Load cleaned data\n",
        "file_path = \"/content/drive/My Drive/TeamFiles/cleaned_data.parquet\"\n",
        "cleaned_df = spark.read.parquet(file_path)\n",
        "\n",
        "print(\"Cleaned data loaded successfully!\")\n",
        "cleaned_df.show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "# Add row_id BEFORE train-test split\n",
        "cleaned_df = cleaned_df.withColumn(\"row_id\", monotonically_increasing_id())\n",
        "\n",
        "# Show first few rows to verify row_id\n",
        "cleaned_df.show(5)\n"
      ],
      "metadata": {
        "id": "1DuaheaamXCG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ce98cce-ac25-48ba-b7df-17a7899c6774"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+----+-------------+----------+-----------------+-------------+---------------------------+----+-----+---+----------+\n",
            "|FSA|      DATE|HOUR|CUSTOMER_TYPE|PRICE_PLAN|TOTAL_CONSUMPTION|PREMISE_COUNT|AVG_CONSUMPTION_PER_PREMISE|YEAR|MONTH|DAY|    row_id|\n",
            "+---+----------+----+-------------+----------+-----------------+-------------+---------------------------+----+-----+---+----------+\n",
            "|M1V|2024-07-01|   1|    SGS <50kW|    Tiered|           3125.6|         1716|         1.8214452214452215|2024|    7|  1|8589934592|\n",
            "|L7M|2024-07-01|   1|    SGS <50kW|       TOU|           1563.8|          683|         2.2896046852122987|2024|    7|  1|8589934593|\n",
            "|L9W|2024-07-01|   1|  Residential|    Tiered|            925.0|         1232|         0.7508116883116883|2024|    7|  1|8589934594|\n",
            "|M2J|2024-07-01|   1|    SGS <50kW|    Tiered|            598.0|          267|         2.2397003745318353|2024|    7|  1|8589934595|\n",
            "|K2C|2024-07-01|   1|  Residential|       TOU|           5095.2|         7714|         0.6605133523463832|2024|    7|  1|8589934596|\n",
            "+---+----------+----+-------------+----------+-----------------+-------------+---------------------------+----+-----+---+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Encode categorical columns\n",
        "customer_type_indexer = StringIndexer(inputCol=\"CUSTOMER_TYPE\", outputCol=\"CUSTOMER_TYPE_INDEX\")\n",
        "price_plan_indexer = StringIndexer(inputCol=\"PRICE_PLAN\", outputCol=\"PRICE_PLAN_INDEX\")\n",
        "\n",
        "# Assemble feature columns (keeping all important features)\n",
        "feature_columns = [\n",
        "    \"HOUR\", \"PREMISE_COUNT\", \"AVG_CONSUMPTION_PER_PREMISE\",\n",
        "    \"CUSTOMER_TYPE_INDEX\", \"PRICE_PLAN_INDEX\", \"YEAR\", \"MONTH\", \"DAY\"\n",
        "]\n",
        "\n",
        "# Assemble & scale features\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
        "\n",
        "# Create a pipeline\n",
        "pipeline = Pipeline(stages=[customer_type_indexer, price_plan_indexer, assembler, scaler])\n",
        "\n",
        "# Apply transformations\n",
        "prepared_data = pipeline.fit(cleaned_df).transform(cleaned_df)\n",
        "\n",
        "print(\"Data successfully transformed for prediction!\")\n",
        "prepared_data.show(5)\n"
      ],
      "metadata": {
        "id": "wh7zZmqimaWa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a2deb54-b0b5-4064-f992-f779d0cedece"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully transformed for prediction!\n",
            "+---+----------+----+-------------+----------+-----------------+-------------+---------------------------+----+-----+---+----------+-------------------+----------------+--------------------+--------------------+\n",
            "|FSA|      DATE|HOUR|CUSTOMER_TYPE|PRICE_PLAN|TOTAL_CONSUMPTION|PREMISE_COUNT|AVG_CONSUMPTION_PER_PREMISE|YEAR|MONTH|DAY|    row_id|CUSTOMER_TYPE_INDEX|PRICE_PLAN_INDEX|            features|     scaled_features|\n",
            "+---+----------+----+-------------+----------+-----------------+-------------+---------------------------+----+-----+---+----------+-------------------+----------------+--------------------+--------------------+\n",
            "|M1V|2024-07-01|   1|    SGS <50kW|    Tiered|           3125.6|         1716|         1.8214452214452215|2024|    7|  1|8589934592|                1.0|             1.0|[1.0,1716.0,1.821...|[0.0,0.0118100395...|\n",
            "|L7M|2024-07-01|   1|    SGS <50kW|       TOU|           1563.8|          683|         2.2896046852122987|2024|    7|  1|8589934593|                1.0|             0.0|[1.0,683.0,2.2896...|[0.0,0.0046379226...|\n",
            "|L9W|2024-07-01|   1|  Residential|    Tiered|            925.0|         1232|         0.7508116883116883|2024|    7|  1|8589934594|                0.0|             1.0|[1.0,1232.0,0.750...|[0.0,0.0084496285...|\n",
            "|M2J|2024-07-01|   1|    SGS <50kW|    Tiered|            598.0|          267|         2.2397003745318353|2024|    7|  1|8589934595|                1.0|             1.0|[1.0,267.0,2.2397...|[0.0,0.0017496354...|\n",
            "|K2C|2024-07-01|   1|  Residential|       TOU|           5095.2|         7714|         0.6605133523463832|2024|    7|  1|8589934596|                0.0|             0.0|[1.0,7714.0,0.660...|(8,[1,2,5,6],[0.0...|\n",
            "+---+----------+----+-------------+----------+-----------------+-------------+---------------------------+----+-----+---+----------+-------------------+----------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prepared_data.write.partitionBy(\"MONTH\").mode(\"overwrite\").parquet(\"partitioned_data\")"
      ],
      "metadata": {
        "id": "3PUoOS-9R9Lk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove any previous Spark installation\n",
        "!rm -rf spark-3.5.4-bin-hadoop3\n",
        "!rm -rf /usr/local/lib/python*/dist-packages/pyspark\n",
        "!rm -rf /usr/local/lib/python*/dist-packages/py4j\n",
        "\n",
        "# Install Spark and Java again\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
        "!tar xf spark-3.5.4-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.4-bin-hadoop3\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Kz0Zu7rokQm",
        "outputId": "8e4d3335-49b4-4e1e-f827-419804cfde46"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Start a new Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PartitionedPredictions\") \\\n",
        "    .config(\"spark.master\", \"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark restarted successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7pz7BSTopBU",
        "outputId": "b5d6e364-6c6b-43be-db8e-ee376b56765d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark restarted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8_SwHUMo-IS",
        "outputId": "30bb9e5f-fd12-4d36-86b8-c90bea24b251"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x7ea28c36d910>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_partition = \"partitioned_data/MONTH=11\"\n",
        "\n",
        "try:\n",
        "    print(f\"Trying to load: {test_partition}\")\n",
        "    prepared_data = spark.read.format(\"parquet\").load(test_partition)\n",
        "    prepared_data.show(5)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading partition: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ozpfdn__o1zC",
        "outputId": "6a727662-f835-4218-eb50-8fa37dc9b4c7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trying to load: partitioned_data/MONTH=11\n",
            "+---+----------+----+-------------+----------+-----------------+-------------+---------------------------+----+---+-----------+-------------------+----------------+--------------------+--------------------+\n",
            "|FSA|      DATE|HOUR|CUSTOMER_TYPE|PRICE_PLAN|TOTAL_CONSUMPTION|PREMISE_COUNT|AVG_CONSUMPTION_PER_PREMISE|YEAR|DAY|     row_id|CUSTOMER_TYPE_INDEX|PRICE_PLAN_INDEX|            features|     scaled_features|\n",
            "+---+----------+----+-------------+----------+-----------------+-------------+---------------------------+----+---+-----------+-------------------+----------------+--------------------+--------------------+\n",
            "|N1G|2024-11-23|   8|    SGS <50kW|       TOU|           1881.1|          627|         3.0001594896331736|2024| 23|34360756377|                1.0|             0.0|[8.0,627.0,3.0001...|[0.30434782608695...|\n",
            "|K2H|2024-11-16|  24|  Residential|       TOU|           7146.9|         9226|         0.7746477346629091|2024| 16|34360465529|                0.0|             0.0|[24.0,9226.0,0.77...|[1.0,0.0639519544...|\n",
            "|K9J|2024-11-23|   8|    SGS <50kW|       TOU|           5093.5|         1860|         2.7384408602150536|2024| 23|34360756378|                1.0|             0.0|[8.0,1860.0,2.738...|[0.30434782608695...|\n",
            "|L3C|2024-11-16|  24|  Residential|       TOU|           9327.4|        12499|         0.7462517001360108|2024| 16|34360465530|                0.0|             0.0|[24.0,12499.0,0.7...|[1.0,0.0866763868...|\n",
            "|L7B|2024-11-23|   8|    SGS <50kW|       TOU|           1141.3|          301|         3.7916943521594684|2024| 23|34360756379|                1.0|             0.0|[8.0,301.0,3.7916...|[0.30434782608695...|\n",
            "+---+----------+----+-------------+----------+-----------------+-------------+---------------------------+----+---+-----------+-------------------+----------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Path where partitions are stored\n",
        "partitioned_data_path = \"partitioned_data\"\n",
        "\n",
        "# Get the list of available partitions\n",
        "available_partitions = [folder for folder in os.listdir(partitioned_data_path) if folder.startswith(\"MONTH=\")]\n",
        "available_partitions.sort()  # Sort to maintain order\n",
        "\n",
        "print(f\"Found partitions: {available_partitions}\")\n",
        "\n",
        "# Initialize an empty list to store Pandas DataFrames\n",
        "all_prepared_pandas = []\n",
        "\n",
        "# Load each partition separately\n",
        "for partition_folder in available_partitions:\n",
        "    partition_path = f\"{partitioned_data_path}/{partition_folder}\"\n",
        "\n",
        "    print(f\"Loading partition: {partition_path}\")\n",
        "\n",
        "    try:\n",
        "        # Read partitioned data from Spark\n",
        "        prepared_data = spark.read.format(\"parquet\").load(partition_path)\n",
        "\n",
        "        # Convert Spark DataFrame to Pandas\n",
        "        prepared_pandas = prepared_data.select(\n",
        "            \"row_id\", \"FSA\", \"DATE\", \"HOUR\", \"CUSTOMER_TYPE\", \"PRICE_PLAN\",\n",
        "            \"PREMISE_COUNT\", \"TOTAL_CONSUMPTION\", \"scaled_features\"\n",
        "        ).toPandas()\n",
        "\n",
        "        # Store in the list for merging later\n",
        "        all_prepared_pandas.append(prepared_pandas)\n",
        "\n",
        "        print(f\"Successfully loaded partition: {partition_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing partition {partition_path}: {e}\")\n",
        "\n",
        "# Merge all partitions into one DataFrame (if needed)\n",
        "if all_prepared_pandas:\n",
        "    prepared_pandas = pd.concat(all_prepared_pandas, ignore_index=True)\n",
        "    X_full = np.array(prepared_pandas[\"scaled_features\"].tolist())\n",
        "    print(f\"Data ready for prediction! Shape: {X_full.shape}\")\n",
        "else:\n",
        "    prepared_pandas = pd.DataFrame()\n",
        "    print(\"No valid partitions found!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Cup45xppEvD",
        "outputId": "b5a4210a-9f8c-4ee6-fd3a-a4fcca7b71f6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found partitions: ['MONTH=11', 'MONTH=4', 'MONTH=7']\n",
            "Loading partition: partitioned_data/MONTH=11\n",
            "Successfully loaded partition: partitioned_data/MONTH=11\n",
            "Loading partition: partitioned_data/MONTH=4\n",
            "Successfully loaded partition: partitioned_data/MONTH=4\n",
            "Loading partition: partitioned_data/MONTH=7\n",
            "Successfully loaded partition: partitioned_data/MONTH=7\n",
            "Data ready for prediction! Shape: (4180454, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Convert features into NumPy array\n",
        "X_full = np.array(prepared_pandas[\"scaled_features\"].tolist(), dtype=np.float32)\n",
        "\n",
        "print(f\"Feature array shape: {X_full.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGtLbuINpMEZ",
        "outputId": "c6fa40e0-6a42-495d-f640-a95061b6ed48"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature array shape: (4180454, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best trained model\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "# Define the custom metric\n",
        "def mse(y_true, y_pred):\n",
        "    return K.mean(K.square(y_pred - y_true), axis=-1)\n",
        "\n",
        "# Load the pre-trained best model\n",
        "best_model_path = \"/content/drive/My Drive/TeamFiles/best_neural_network.h5\"\n",
        "best_model = tf.keras.models.load_model(best_model_path, custom_objects={\"mse\": mse})\n",
        "print(\"Best model reloaded successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctqXtRMUpQqP",
        "outputId": "0b37f3b2-8a8e-46a2-d5a0-2530b889d595"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model reloaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "y_full_pred = best_model.predict(X_full)\n",
        "\n",
        "# Convert predictions to Pandas DataFrame\n",
        "predictions_pandas = pd.DataFrame({\n",
        "    \"row_id\": prepared_pandas[\"row_id\"],\n",
        "    \"PREDICTED_CONSUMPTION\": y_full_pred.flatten()\n",
        "})\n",
        "\n",
        "print(\"Predictions successfully generated!\")\n",
        "print(predictions_pandas.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvRmCOqDpVgd",
        "outputId": "d0f07e2b-0a8a-4f2c-bbd2-dcd70f9b7dff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m130640/130640\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 1ms/step\n",
            "Predictions successfully generated!\n",
            "        row_id  PREDICTED_CONSUMPTION\n",
            "0  34360756377            1864.750000\n",
            "1  34360465529            7268.404297\n",
            "2  34360756378            5176.888672\n",
            "3  34360465530            9463.594727\n",
            "4  34360756379            1097.583130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge predictions with prepared data\n",
        "merged_pandas = prepared_pandas.merge(predictions_pandas, on=\"row_id\", how=\"left\")\n",
        "\n",
        "# Convert back to Spark DataFrame\n",
        "final_cleaned_df = spark.createDataFrame(merged_pandas)\n",
        "\n",
        "print(\"Predictions merged successfully into the dataset!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WKccztIpaH3",
        "outputId": "36e61667-baa3-4d95-ebe4-fbaf7178bce8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions merged successfully into the dataset!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "J-cOXQJBqwYi",
        "outputId": "430b9ed6-a9a2-474f-8a95-533e2295b760"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m144\u001b[0m)                 │           \u001b[38;5;34m1,296\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m240\u001b[0m)                 │          \u001b[38;5;34m34,800\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m)                  │          \u001b[38;5;34m11,568\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m240\u001b[0m)                 │          \u001b[38;5;34m11,760\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m176\u001b[0m)                 │          \u001b[38;5;34m42,416\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m208\u001b[0m)                 │          \u001b[38;5;34m36,816\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m209\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">144</span>)                 │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,296</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">34,800</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">11,568</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">240</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">11,760</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">176</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">42,416</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">208</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,816</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">209</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m138,867\u001b[0m (542.45 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">138,867</span> (542.45 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m138,865\u001b[0m (542.44 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">138,865</span> (542.44 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_cleaned_df.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfwXDq6Uw11I",
        "outputId": "c5fde1b1-c8d0-4dff-b539-182cb3975e84"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- row_id: long (nullable = true)\n",
            " |-- FSA: string (nullable = true)\n",
            " |-- DATE: date (nullable = true)\n",
            " |-- HOUR: long (nullable = true)\n",
            " |-- CUSTOMER_TYPE: string (nullable = true)\n",
            " |-- PRICE_PLAN: string (nullable = true)\n",
            " |-- PREMISE_COUNT: long (nullable = true)\n",
            " |-- TOTAL_CONSUMPTION: double (nullable = true)\n",
            " |-- scaled_features: vector (nullable = true)\n",
            " |-- PREDICTED_CONSUMPTION: double (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ai0-gamLw280"
      },
      "execution_count": 17,
      "outputs": []
    }
  ]
}